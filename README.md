<a href="https://pandora-llm.readthedocs.io/en/latest/"><img alt="Documentation" src="https://img.shields.io/website?url=https%3A%2F%2Fpandora-llm.readthedocs.io%2Fen%2Flatest%2F&up_message=sphinx&label=docs&color=blue"></a>
<a href="https://pypi.org/project/pandora-llm/"><img alt="Python version" src="https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Fsafr-ai-lab%2Fpandora_llm%2Frefs%2Fheads%2Fmain%2Fpyproject.toml&color=green"></a>
<a href="https://github.com/safr-ai-lab/pandora_llm/blob/main/LICENSE.txt"><img alt="Code license" src="https://img.shields.io/github/license/safr-ai-lab/pandora_llm?color=blue"></a>
<a href="https://github.com/safr-ai-lab/pandora_llm/releases"><img alt="GitHub release" src="https://img.shields.io/github/v/release/safr-ai-lab/pandora_llm?color=green"></a>

<p align="center">
   <img src="docs/source/assets/pandora_llm_title.png" alt="Pandora LLM" width="600"/>
</p>

## Overview

`pandora_llm` is a red-teaming library against Large Language Models (LLMs) that assesses their vulnerability to train data leakage.

It provides a unified [PyTorch](https://pytorch.org/) API for evaluating **membership inference attacks (MIAs)**.

Please refer to the [documentation](https://pandora-llm.readthedocs.io/en/latest/) for the API reference as well as tutorials on how to use this codebase.

`pandora_llm` abides by the following core principles:

- **Open Access** — Ensuring that these tools are open-source for all.
- **Reproducible** — Committing to providing all necessary code details to ensure replicability.
- **Self-Contained** — Designing attacks that are self-contained, making it transparent to understand the workings of the method without having to peer through the entire codebase or unnecessary levels of abstraction, and making it easy to contribute new code.
- **Model-Agnostic** — Supporting any [HuggingFace](https://huggingface.co/) model and dataset, making it easy to apply to any situation.
- **Usability** — Prioritizing easy-to-use starter scripts and comprehensive documentation so anyone can effectively use `pandora_llm` regardless of prior background.

We hope that our package serves to guide LLM providers to safety-check their models before release, and to empower the public to hold them accountable to their use of data.

## Installation

From pip:
```
pip install pandora-llm
```

From source:

```bash
git clone https://github.com/safr-ai-lab/pandora-llm.git
pip install -e .
```

## Quickstart

You can run a membership inference attack through the shortcut `pandora-mia`, which is an alias for `python -m pandora_llm.routines.membership_inference`.

This API takes several important command-line arguments:

- `--dataset.name`: the HuggingFace dataset
- `--dataset.num_train_samples`: number of train samples
- `--dataset.num_val_samples`: number of validation samples
- `--dataset.train_start_index`: index to start taking train samples
- `--dataset.val_start_index`: index to start taking val samples
- `--features.names`: features to use
- `--classifier.name`: classifier to use
- `--model.name`: the HuggingFace language model name
- `--features.compute`: whether to compute the features
- `--classifier.train`: whether to train a classifier
- `--classifier.infer`: whether to run inference

Some attacks will require additional arguments, please refer to the documentation for each attack for more details.

We maintain a collection of minimal example API invocations in our codebase under ``scripts/examples.sh``.

For more information on how to format the HuggingFace dataset, please see `Building with Pandora` in the documentation.

Clean splits of The Pile and Dolma can be generated by the routines in the ``scripts/data_generation`` directory.

## Contributing
We welcome contributions! Please submit pull requests in our [GitHub](https://github.com/safr-ai-lab/pandora-llm).

Please see `Building with Pandora` in the documentation for more details on how to contribute to the library.

## Authors

This library was created by Jeffrey G. Wang, Jason Wang, Marvin Li, and Seth Neel.
