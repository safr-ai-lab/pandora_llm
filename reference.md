## Thanks To

If you add any open-source models/datasets/libraries, please consider appending them here!

Open-Source Models: [Pythia](https://github.com/EleutherAI/pythia) (Apache License, Version 2.0) and [Llama-2](https://llama.meta.com/llama2/) ([License](https://ai.meta.com/llama/license/)).
```
Biderman, Stella, et al. "Pythia: A suite for analyzing large language models across training and scaling." International Conference on Machine Learning. PMLR, 2023.

Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." arXiv preprint arXiv:2307.09288 (2023).
```

Open-Source Datasets: EleutherAI's [Pile](https://github.com/EleutherAI/the-pile) Dataset (MIT License).
```
Gao, Leo, et al. "The pile: An 800gb dataset of diverse text for language modeling." arXiv preprint arXiv:2101.00027 (2020).
```

Open-Source Libraries: [PyTorch](https://pytorch.org/) ([License](https://github.com/pytorch/pytorch/blob/main/LICENSE)) and [Huggingface](https://huggingface.co/) (Apache License 2.0).
```
Ansel, Jason, et al. "PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation." Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. 2024.

Wolf, Thomas, et al. "Huggingface's transformers: State-of-the-art natural language processing." arXiv preprint arXiv:1910.03771 (2019).
```
